---
layout: post
title: "Get Started with Dask"
date: 2021-08-02
---
<span class="dropcap">D</span>ask is simply awesome. If you are looking for Spark alternatives that are light-weight, simple, easy to set-up, native to Python, and do its job (i.e distributed computing) well without changing much of your existing Python codes (in `numpy` or `pandas` or whatever), you absolutely should give Dask a try!

First and foremost, the ultimate guide you should refer to is the [official doc](https://docs.dask.org/en/latest/install.html).

## Local set-up
Recommend to use `conda` to set up the local environment quickly.

- `conda env create -f environment.yml`
- `conda activate dask-intro`
- If local:
  
    ```Python
    from dask.distributed import Client
    client = Client(n_workers=4)
    ```


## Performance benchmark
### Data
Following a classic example in Dask's tutorials, I will use this [NYC Yellow Taxi Trips Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) to benchmark performance. This is a medium-sized data set which probably cannot fit into RAM but can fit into most machines' physical storage. In the flip side, if the data can fit into the RAM then probably you don't need Dask or tooling to do distributed computing. 
Let's download 2019 Yellow Taxi Trip Records Oct, Nov, Dec data into the local machine and check the memory footprint first.

<figure>
    <img src="{{ '/assets/img/dask-df.png' | prepend: site.baseurl }}" alt="">
</figure>

### Raw Pandas
<figure>
    <img src="{{ '/assets/img/dask-pd.png' | prepend: site.baseurl }}" alt="">
</figure>

### Local 4-core
It's super easy to set-up a local distributed cluster using Dask. Dask is no much different than Python process or threads parallel computing library that you may already use in the past, such as [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html).
However, it provides great convenience setting up computing graphs, scheduling tasks, parallel computing optimization under the hood. The computing mechanism also looks similar to Spark, such as lazy evaluation, scaling up to massive number of cores in cloud etc.

<figure>
    <img src="{{ '/assets/img/dask-overview.svg' | prepend: site.baseurl }}" alt="">
</figure>


```python
# A simple computation over 6.6G data
%%time
import dask.dataframe as dd

df = dd.read_csv("~/Documents/GitHub/data/Taxi/yellow_tripdata_2019-*.csv",
                 dtype={'RatecodeID': 'float64',
                        'VendorID': 'float64',
                        'passenger_count': 'float64',
                        'payment_type': 'float64',
                        'tolls_amount': 'float64',
                        'store_and_fwd_flag': 'object'
                       })

```
Note that Dask estimates the data types with a small sample of data to stay efficient, so a good practice is to specify data types during `read_csv()`.
Up to this point, nothing actually happens except for the task graphs are set up. Now it's time actually kicking off the computation by calling `.compute()`

```python
%%time
mean_tip_amount = df.groupby("passenger_count").tip_amount.mean()
mean_tip_amount.compute()
```
<figure>
    <img src="{{ '/assets/img/dask-local.png' | prepend: site.baseurl }}" alt="">
</figure>

### Go to Cloud
To take advantage of the free tier [Coiled](https://coiled.io/) cluster service I will benchmark the performance on the Cloud there. Let's do the same thing on the 10-worker cloud.
<figure>
    <img src="{{ '/assets/img/dask-cloud.png' | prepend: site.baseurl }}" alt="">
</figure>

Astoundingly it reduces to wall clock time from 4min 34s to merely 43.6s, just adding a few extra lines of code to your original Pandas codes. Of course this performance estimate is pretty *crude*. Actually there is tons of room to improve the raw Pandas way such as only reading the columns that you need.