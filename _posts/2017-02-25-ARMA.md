---
layout: post
title:  "Linear regression with ARMA errors"
date:   2017-02-25
---

<span class="dropcap">R</span>ecently I'm getting more and more interested in time series prediction, which might be somehow neglected by the machine learning community. However, this topic should have attracted massive attention --- who doesn't want to know (even get a bit of sense) tomorrow's stock price? Obviously, this type of question is difficult. People actually even do not know if it is *possible* to solve. The author of *"A Random Walk to Wall Street"* even claimed that "a blindfolded monkey throwing darts at a newspaper’s financial pages could select a portfolio that would do just as well as one carefully selected by experts.” The prevailing [theory](https://en.wikipedia.org/wiki/Efficient-market_hypothesis) says that stock price is totally random and unpredictable. So this post just demonstrates one way to model time series data where it applies. For example, economists find some models are proved to be meaningful and useful in studying the relationship between macro-economical indexes.

Let's get back to this post's main theme: multivariate time series prediction. A decent example would be like that predicting next 5 year's bank loan's default rate given next 5 year's macro-economical variables such as unemployment rate, prime rate, GDP etc. Here I'm going to show how to build linear regression with non-seasonal ARMA errors

## Workflow for time series data
- detect outliers
- detect trend
- detect seasonality
- detect long-run cycle (assuming constant variance)

If you are using R, I will recommend function `TS:::decompose` to decompose time series where it provides two ways: additive decomposition and multiplicative decomposition. 

## Model
You may want to fit an ordinary linear regression on the time-series data. However, not surprisingly, the resulting model's residuals have a time series structure since the `\(i.i.d\)` assumption does not hold. Therefore the estimated coefficients and confidence intervals are not reliable.
To address the issue, we should build linear regression model with ARIMA errors.

A non-seasonal linear regression model with ARIMA `\(p, d, q\)` errors can be writeen as:

$$y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + .... + e_t$$

where
$$(1-\phi_1B – \cdots – \phi_p B^p)(1-B)^d e_t^  =  c + (1 + \theta_1 B + \cdots + \theta_q B^q)w_t $$

where `\(B\)` is the backshift operator, `\( c = \mu(1-\phi_1-\phi_2-...-\phi_p)   w_t \sim iid \text{N}(0, \sigma^2) \)`


## `\(H\)` step-ahead time series forecasting strategies
When you try to provide *long* time forecasts with limited sample size, you have to be careful about the strategy of choice. I found out this [paper](http://www.sciencedirect.com/science/article/pii/S0957417412000528) gives a comprehensive review of `\(H\)` step-ahead time series forecasting strategies.
In the example I'm going to show you, I adopted the so-called *"recursive strategy"* where we optimize a model based on a one-step ahead criterion. When calculating a `\(H\)`-step ahead forecast, we iteratively feed the forecasts of the model back in as input for the next prediction. Of course, there are many other strategies out there and "Multiple-Output strategies are the best performing approaches". 

Firstly, a single model `\(f\)` is trained to perform a *one-step ahead* forecast. 
`\[
y_{t+1} = f(y_t, ..., y_{t-p+1}) + \epsilon_t
\]`

The advantage of this method is it returns the **unbiased** estimator of  

$$\mathbb{E}\left[ \textbf{y}_{(t+1):(t+H)} \,\vert\, \textbf{y}_{(t-d+1):t} \right]$$ 

where

`\[  
\textbf{y}_{(t+1):(t+H)} = [y_{t + 1}, \ldots, y_{t + H}] \in \mathbb{R}^H
 \]`
and
`\[ 
\textbf{y}_{(t-d+1):t} = [y_{t - d + 1}, \ldots, y_t] \in \mathbb{R}^d
 \]`


Another advantage is that we need to train only one model.


## Implementation in R
Unfortunately, I was unable to find any on-line R function to give me the predictions applying the recursive strategy. So I wrote my own.  
```R
####### prepare all combinations parameters ##############
# para <- dataframe(p=c(0,5), d=0, q=c(0,3));
# xreg is a vector or matrix of external regressors
# resp: response variable
# training/testing data
# vars: variables names

#########################################################
#### Non-seasonal No-differencing
#### One-step-ahead prediction
#########################################################

get_one_step <- function(old = old, new = new, para_id = para_id){
  
  ### Model #######################
  p <- para[para_id, 'p'];
  d <- para[para_id, 'd'];
  q <- para[para_id, 'q'];
  
  
  reg <- arima (train_y,
                order = c(p, d, q),
                xreg = xreg
                );
  
  ## Data ########################
  
  old <- old[order(old$time), ];
  new <- new[order(new$time), ];
  current_mth <- head(new, 1)$time;
  
  
  if(d != 0){
    stop("Currently I cannot handle with the situation where d>0")
  } else {
    if (p >0) {
      
      old_ <- tail(old, p)[, c(resp, vars)];
      new_ <- head(new, 1)[vars];
      
      
      ar <- reg$coef[grep("ar", names(reg$coef))];
      
      ar_part <- ar %*% rev(old_[, resp]);
      
      beta_0 <- (1-sum(ar))*reg$coef[["intercept"]];
      
      beta_t <- ifelse(any(names(reg$coef) %in% 'x_trend'),
                       new_[, 'trend']*reg$coef[["x_trend"]],
                       0);
      
      
      beta_v <- rep(NA, length(colnames(xreg)));
      
      for(v in c(1: length(colnames(xreg))) ){
        
        vv <- colnames(xreg)[v];
        vv_nox <- gsub('x_', '', vv);
        
        beta_v[v] <- reg$coef[[vv]]*
          (new_[, vv_nox] - (ar %*% rev(old_[, vv_nox])) );
        
        if(vv_nox == 'trend' | vv == 'x_trend'){ beta_v[v] <- 0; }
        # don't double compute trend part
      }
      
      # try the one step ahead prediction
      pred_one <- new_;
      pred_one[, 'time'] <- current_mth;
      pred_one[, resp] <- sum(ar_part, beta_0, beta_t) + sum(beta_v);
      
      
      return(pred_one)
    }
    else {
      stop("p should be >0!")
      
    }
  }
  
}


### Model performance ################################
get_SMAPE <- function(para_id = para_id){
  
  pred <- data.frame(NA);
  
  for(h in c(1:H)){
    
    if(h == 1){
      old <- training;
      new <- testing;
      pred <- get_one_step(old = old, new = new, para_id = para_id);
      
    }else{
      old <- rbind(pred[(h-1), ], old);
      new <- subset(new, new$time > max(pred$time)) ;
      
      ## check
      if(nrow(old) + nrow(new) != (nrow(training)+nrow(testing)) ){
        stop('Total rows cannot be matched!')
      }
      
      pred[h, ] <- get_one_step(old = old, new = new, para_id = para_id);
      
    }
  }
  
  
  return(
    list(
      MAPE = mean((abs(test_y-pred[, resp])/test_y)*100),
      SMAPE = mean((2*abs(test_y-pred[, resp])/(test_y+pred[, resp]))*100)
    )
  );
}



# if there is no AR part, just MA part
get_SMAPE_MA <- function(para_id = para_id){
  
  p <- para[para_id, 'p'];
  d <- para[para_id, 'd'];
  q <- para[para_id, 'q'];
  
  
  reg <- arima (train_y,
                order = c(p, d, q),
                xreg = xreg
                );

  m1 <- matrix(tail(reg[['coef']], (ncol(xreg)+1)), ncol = 1);
  temp <- testing[ , -which(names(testing) %in% c("time"))];
  m2 <- as.matrix(cbind(intercept = rep(1, nrow(testing)), temp));
  pred <- m2%*%m1;
  
  return(
    list(
      MAPE = mean((abs(test_y-pred)/test_y)*100),
      SMAPE = mean((2*abs(test_y-pred)/(test_y+pred))*100)
    )
  )
  
}

#### Results ######################################
acc <- lapply(X = 1:nrow(para), 
              FUN = function(x){
                if(para[x, 'p'] >0){
                  unlist(get_SMAPE(x));
                } else { unlist(get_SMAPE_MA(x)) }
                }
            );

final <- as.data.frame(t(matrix(unlist(acc), ncol = nrow(para))));
colnames(final) <- c('MAPE', 'SMAPE');
final$id <- c(1:nrow(para));
para$id <- c(1:nrow(para));
final <- merge(final, para, by = 'id');
final[, c(1:6)];
```

Here two forecast accuracy matrics, `\(MAPE\)`
 and `\(SMAPE\)` are returned, contained in the `final` dataframe.

`\[
\text{MAPE} = 100\text{mean}(|y_t – \hat{y}_t|/|y_t|) 
\]`

Because `\( \text{MAPE} \)` puts a heavier penalty on negative errors (when `\(y_<\hat{y}_t\)`) than on positive errors. To avoid the asymmetry of `\( \text{MAPE}\)`, we should also consider the Symmetric MAPE (SMAPE).

`\[
 \text{sMAPE} = 100\text{mean}(2|y_t – \hat{y}_t|/|y_t + \hat{y}_t|)
\]`

The end-to-end example walk-through can be found [here](https://gist.github.com/HongleiXie/c924768d52669c65b9be424eee8c970b)


## Also try
### Bayesian
Let's forget ARIMA and go Bayesian! Please see this great [post](http://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/).

### Recurrent neural network (RNN)
Just in the same time when I was writing this post, I noticed that Siraj has released a new Youtube [video](https://www.youtube.com/watch?v=ftMq5ps503w&feature=youtu.be) discussing a demo using [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) to predict stock price. LSTM gained a lot of popularity and success in doing NLP tasks. How does it work for time series? I'm very exceited to see the potentials of deep learning in solving traitional modelling problem including time series prediction.
BTW, [Siraj](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/about) is the best story-teller machine learninger/video educator that I have seen so far. Highly recommend his Youtube channels to you! 
