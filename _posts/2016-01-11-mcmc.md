---
layout: post
title:  "Computation Inference in Logistic Regression"
date:   2016-01-11
---

Logistic regression is one of the most commonly used techniques to analyze binary
data. The classical method to estimate the parameters is through Newton-Rapson. Here I'm demonstrating
the alternative method: Bayesian method (MCMC) and make a comparison based on a real dataset.

The dataset *sediment* is composed of 1884 sediment samples. Samples were divided into 2
classes on the basis of their toxicity (the toxic class is coded as 1 and non toxic class is
coded as 0) and described by 9 chemical variables. Since we are interested in making
statistical inference on parameters, so we only used the training set (1414 samples).

To build the benchmark model, I used `glm` function in R to estimate parameters. 
We know that Newton-Rapson (NR) algorithm is used in this function. However, NR suffers from limitations
such as it requires the Hessian matrix being non-singular and the initial values having
much effect on the convergence rate. Thus, I considered implementing Markov
Chain Monte Carlo (MCMC) to estimate the parameters instead.

Following is the idea of this method:
The proposed model is:

`\[log(\frac{p_i}{1-p_i})=x_i' \beta=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_9x_{i,9}\]`

`\[Y_i{\sim}Bernoulli(p_i)\quad L(\boldsymbol{\beta})= \prod\limits_{i=1}^n  {p_i}^ {y_i} + (1-p_i)^{(1-y_i)}\]`
`\[ l(\boldsymbol{\beta})=\sum\limits_{i=1}^n y_i log(p_i)+(1-y_i)log(1-p_i)\]`

We can prove that `\(Var(\boldsymbol{\beta})=(X'WX)^{-1}\)` where `W` is a diagonal matrix of weights with entries
`\[w_{ii}=\widehat{\mu}(n_i-\widehat{\mu_i})/n_i\]`
Thus, `\(log(P(\beta \vert Y,X))=\sum\limits_{i=1}^n y_i log(p_i)+(1-y_i)log(1-p_i)\)`
Everytime we draw from the `\(\boldsymbol{\beta^{(t)}}\)` that comes from $N(\boldsymbol{\beta^{(t-1)}},\mathbf{H}^{-1})$ where $\mathbf{H}=X'WX$. Here we applied Metropolis-Hastings algorithm to sample $\boldsymbol{\beta}$, which means that acceptance= min$(1,T(\beta^{(t-1)} \vert \beta^{\ast}) P(\beta ^{\ast})\Big/T(\beta^{\ast} \vert \beta^{(t-1)})P(\beta ^{(t-1)}))$. Then we generate $U{\sim}Unif(0,1)$, if $U<$ acceptance, $\beta^{(t)}=\beta^{\ast}$ otherwise $\beta^{(t)}=\beta^{(t-1)}.$

To avoid the the non-invertable matrix problem, we used Cholesky decomposition in our codes; and it turned out to be $H=R'R\quad \beta^{\ast}=R^{-1}q+\beta^{(t-1)}$ where $q{\sim}N(\mathbf{0},\mathbf{I_k})$


There are the results in MCMC implemented in Matlab and Newton-Rapson
approach in `glm` function in R.


### Thoughts of Mine

- Estimates in logistic regression using Newton-Rapson and MCMC algorithm
are very close to each other. It turns out that deterministic method and
Bayesian method have a pretty good agreement in this case.
- The likelihood function converges after about 200 iterations which
shows parameters estimated by MCMC are stable.
- MCMC is essentially a Bayesian method; so it has a few nice properties; for example,
it can handle complex models that deterministic methods cannot solve, at the
expense of high computational cost and relatively slow convergence rate though.
- Newton-Rapson is considered as the *best* method when making inferences
in ordinary logistic regression since it is pretty fast and it has great accuracy. 
Here I just implemented MCMC on ordinary logistic regression to demonstrate that we actually have alternative one to
estimate parameters in the framework of logistic regression instead of routine Newton-Rapson.
Moreover, when it comes to the generalized linear mixed effects model with non-identity
link where there is no closed form for the full likelihood function Bayesian method seems to be the only choice. 

```matlab
% logit_mcmc2.m file %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implement MCMC to fit logistic regression in a real data set
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function w_est = logit_mcmc2(w_init,data,labels,max_iter,lambda)
%assume data == features by samples
%w_init w_init == (features+1) by 1
%labels == samples by 1
w_est = [];
features = size(data,1);
samples = size(data,2);

data_new = [ones(1,samples); data];
features = features + 1;

%data_new = data;
if size(w_init,1) ~= size(data_new,1)
	error('er1')
	return
end
if size(labels,1) ~= samples
	error('er2')
	return
end
uni_label = unique(labels);
if length(uni_label) ~= 2 || length(find(uni_label == 0)) ~= 1 || length(find(uni_label == 1)) ~= 1
	error('er3')
	return
end

cur_iter = 0;
w_est = w_init;

burn_in = 1000;
save =[];

X_iter=[];
Y_iter=[];

trace=[]

while cur_iter < max_iter
	%disp(cur_iter)
	cur_iter = cur_iter + 1;
	tmp1 = data_new' * w_est;%X'*W  samples*1
	predict_old = 1.0 ./ (exp(-tmp1)+1);%Y samples * 1

	assert ( length(find(predict_old > 0)) == samples );
	assert ( length(find(predict_old < 1)) == samples );

	d_old = (predict_old .* (1.0-predict_old));% samples * 1
	assert (size(d_old,1) == samples);
	tmp2 = data_new*sparse(1:samples,1:samples,d_old);%X*R features*samples
	tmp3 = tmp2 * data_new';%X*R*X' features*features
	R_old = chol(tmp3/(lambda*lambda));
	eig(inv(R_old));

	tag = logical((predict_old > 0) .* (predict_old < 1));
	log_like_old = sum(log(predict_old(tag)).*labels(tag)) + sum(log(1.0 - predict_old(tag)).* (1-labels(tag)));

	w_est_new = R_old \ (mvnrnd(zeros(features,1), eye(features))');
	w_est_new = w_est_new + w_est;
	assert(size(w_est_new,1) == size(w_est,1));

	tmp1 = data_new' * w_est_new;%X'*W  samples*1
	predict_new = 1.0 ./ (exp(-tmp1)+1);%Y samples * 1
	d_new = (predict_new .* (1.0-predict_new));% samples * 1
	assert (size(d_new,1) == samples);

	tmp2 = data_new*sparse(1:samples,1:samples,d_new);%X*R features*samples
	tmp3 = tmp2 * data_new';%X*R*X' features*features
	R_new = chol(tmp3/(lambda*lambda));
	eig(inv(R_new));

	tag = logical((predict_new > 0) .* (predict_new < 1));
	log_like_new = sum(log(predict_new(tag)).*labels(tag)) + sum(log(1.0-predict_new(tag)).* (1-labels(tag)));

	assert(det(R_new)> 0)
	assert(det(R_old)> 0)

	tmp = R_new * (w_est - w_est_new);
	log_old_cod_new = log(det(R_new)) - tmp'*tmp*0.5;

	tmp = R_old * (w_est_new - w_est);
	log_new_cod_old = log(det(R_old)) - tmp'*tmp*0.5;

	log_accept = log_like_new - log_like_old + log_old_cod_new - log_new_cod_old;

	if log_accept > 0
		log_accept = 0;
	end
	accept = rand;

	X_iter = [X_iter cur_iter];
	if log(accept) <= log_accept
		w_est = w_est_new;
		%disp('accept')
		set1 = find(predict_new > 0.5);
		set2 = find(labels == 1);
		set3 = intersect(set1, set2);
		errors = length(set1)+length(set2)-2*length(set3);
		%disp(errors/samples);
		Y_iter = [Y_iter log_like_new];
	else
		Y_iter = [Y_iter log_like_old];
	end
	if cur_iter >= burn_in 
		if mod(cur_iter,50) == 0 
			save = [save w_est];
		end
		trace = [trace w_est];
	end

end

w_est = mean(save,2);

num = size(trace,2);
for j = 1:size(trace,1)
	%figure;
	%plot(1:num, trace(j,:))
	figure;
	autocorr(save(j,:))
end

figure;
plot(X_iter,Y_iter)
xlabel('MCMC iteration')
ylabel('data likelihood')

end

%%%%%%% Call logit_mcmc2.m %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

load sediment.mat

X_sample_train = Xtrain;
lable_sample_train = class_train-1;
features = size(X_sample_train,1)+1;

w_init = zeros(features,1);
max_iter = 10000;
lambda = 1;
w_est = logit_mcmc2(w_init,X_sample_train,lable_sample_train,max_iter,lambda);
disp('mcmc')
w_est
```


